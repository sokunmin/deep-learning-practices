{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.estimator package not installed.\n",
      "tf.estimator package not installed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`tf.data`** API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. <br/><br/>The pipeline for a text model might involve extracting symbols from raw text data, converting them to __embedding identifiers__ with a lookup table, and batching together sequences of different lengths. The **`tf.data`** API makes it easy to deal with large amounts of data, different data formats, and complicated transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [**`tf.data`**](https://www.tensorflow.org/api_docs/python/tf/data) API introduces two new abstractions to TensorFlow:\n",
    "\n",
    "1. <font color=blue>A [**`tf.data.Dataset`**](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) represents a sequence of elements, in which each element contains one or more [**`Tensor`**](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects.</font> For example, in an image pipeline, an element might be a single training example, with a pair of tensors representing the image data and a label. There are two distinct ways to create a dataset:\n",
    "\n",
    "    * <font color=blue>Creating a <font color=green>source</font> (e.g. **`Dataset.from_tensor_slices()`**) constructs a dataset from one or more **`tf.Tensor`** objects.</font>\n",
    "\n",
    "    * <font color=blue>Applying a <font color=green>transformation</font> (e.g. **`Dataset.batch()`**) constructs a dataset from one or more **`tf.data.Dataset`** objects.</font>\n",
    "\n",
    "2. <font color=blue>A [**`tf.data.Iterator`**](https://www.tensorflow.org/api_docs/python/tf/data/Iterator) provides the main way to extract elements from a dataset The operation returned by **`Iterator.get_next()`** yields the next element of a **`Dataset`** when executed.</font>, and <font color=red>typically acts as the interface between input pipeline code and your model.</font> <br/><br/>The simplest iterator is a \"<font color=green>one-shot iterator</font>\", <font color=blue>which is associated with a particular **`Dataset`** and iterates through it once.</font> For more sophisticated uses, <font color=blue>the **`Iterator.initializer`** operation enables you to reinitialize and parameterize an iterator with different datasets</font>, so that you can, for example, iterate over training and validation data multiple times in the same program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the guide describes the fundamentals of creating different kinds of **`Dataset`** and **`Iterator`** objects, and how to extract data from them.\n",
    "\n",
    "<font color=blue>To start an input pipeline, you must define a <font color=green>source</font></font>. For example, <font color=blue>to construct a **`Dataset`** from some tensors in memory</font>, you can use **`tf.data.Dataset.from_tensors()`** or **`tf.data.Dataset.from_tensor_slices()`**. Alternatively, <font color=blue>if your input data are on disk in the recommended <font color=green>TFRecord format</font>, you can construct a [**`tf.data.TFRecordDataset`**](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset).</font>\n",
    "\n",
    "<font color=blue>Once you have a **`Dataset`** object, you can <font color=green>transform</font> it into a new **`Dataset`** by chaining method calls on the **`tf.data.Dataset`** object.</font> For example, <font color=blue>you can apply per-element transformations such as **`Dataset.map()`** (to apply a function to each element), and multi-element transformations such as **`Dataset.batch()`**.</font> See the documentation for [**`tf.data.Dataset`**](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for a complete list of transformations.\n",
    "\n",
    "<font color=blue>The most common way to consume values from a **`Dataset`** is to make an iterator object that provides access to one element of the dataset at a time (for example, by calling **`Dataset.make_one_shot_iterator()`**).</font> <br/><br/>A **`tf.data.Iterator`** provides two operations: \n",
    "* <font color=blue>**`Iterator.initializer`**, which enables you to (re)initialize the iterator's state; </font>\n",
    "* <font color=blue>**`Iterator.get_next()`**, which returns **`tf.Tensor`** objects that correspond to the symbolic next element.</font> Depending on your use case, you might choose a different type of iterator, and the options are outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Dataset structure</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Dataset</font>: <font color=blue>A dataset comprises elements that each have the same structure.</font> <br/><br/>\n",
    "<font color=green>Element</font>: <font color=blue>an element contains one or more **`tf.Tensor`** objects, called <font color=green>components</font></font>. <br/><br/>\n",
    "<font color=green>Component</font>: <font color=blue>each component has a **`tf.DType`** representing the type of elements in the tensor, and a **`tf.TensorShape`** representing the (possibly partially specified) static shape of each element.</font></font> <br/><br/>The **`Dataset.output_types`** and **`Dataset.output_shapes`** properties <font color=blue>allow you to inspect the inferred types and shapes of each <font color=green>component</font> of a dataset <font color=green>element</font>.</font> <br/><br/>The <font color=green>nested structure</font> of these properties <font color=blue>map to the structure of an element, which may be a single tensor, a tuple of tensors, or a nested tuple of tensors.</font> <br/><br/>For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n",
      "\n",
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "\n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "print()\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "print()\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often convenient to give names to each <font color=green>component</font> of an <font color=green>element</font>, for example if they represent different features of a training example. <br/><br/><font color=blue>In addition to tuples, you can use **`collections.namedtuple`** or a **`dictionary`** <font color=red>_mapping strings to tensors_</font> to represent a single <font color=green>element</font> of a **`Dataset`**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tf.float32, 'b': tf.int32}\n",
      "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>The **`Dataset`** <font color=green>_transformations_</font> support datasets of any structure. <br/><br/>When using the **`Dataset.map()`**, **`Dataset.flat_map()`**, and **`Dataset.filter()`** <font color=green>_transformations_</font>, which apply a function to each <font color=green>_element_</font>, the <font color=green>_element_</font> structure determines the arguments of the function</font>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset1.map(lambda x: ...)\n",
    "\n",
    "dataset2 = dataset2.flat_map(lambda x, y: ...)\n",
    "\n",
    "# Note: Argument destructuring is not available in Python 3.\n",
    "dataset3 = dataset3.filter(lambda x, (y, z): ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Creating an iterator</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have built a **`Dataset`** to represent your input data, the next step is to create an **`Iterator`** to <font color=blue>access elements from that dataset.</font> The **`tf.data`** API currently supports the following iterators, in increasing level of sophistication:\n",
    "<font color=green>\n",
    "* **one-shot iterator**\n",
    "* **initializable iterator**\n",
    "* **reinitializable iterator**\n",
    "* **feedable iterator**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>◎ one-shot iterator</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <font color=green>**one-shot iterator**</font> is the simplest form of iterator, which only <font color=blue>supports iterating once through a dataset</font>, <font color=red>with no need for explicit initialization.</font> <br/><br/><font color=blue><font color=green>**One-shot iterators**</font> handle almost all of the cases that the existing queue-based input pipelines support,</font> <font color=red>but they do not support _*parameterization*_.</font> <br/><br/>Using the example of **`Dataset.range()`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    dataset = tf.data.Dataset.range(100)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    \n",
    "    result = []\n",
    "    for i in range(100):\n",
    "        value = sess.run(next_element)\n",
    "        result.append(str(value))\n",
    "        assert i == value\n",
    "    print('result:', ','.join(result))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>◎ initializable iterator</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An <font color=green>**initializable iterator**</font> <font color=blue>requires you to run an explicit **`iterator.initializer`** operation before using it</font>. <br/><br/>In exchange for this inconvenience, <font color=blue>it enables you to <font color=red>_parameterize_</font> the definition of the dataset, using one or more **`tf.placeholder()`** tensors that can be fed when you initialize the iterator.</font> <br/><br/>Continuing the **`Dataset.range()`** example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 elements:  0,1,2,3,4\n",
      "10 elements:  0,1,2,3,4,5,6,7,8,9\n"
     ]
    }
   ],
   "source": [
    "max_value    = tf.placeholder(tf.int64, shape=[])\n",
    "dataset      = tf.data.Dataset.range(max_value)\n",
    "iterator     = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = []\n",
    "    # Initialize an iterator over a dataset with 5 elements.\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 5})\n",
    "    for i in range(5):\n",
    "        value = sess.run(next_element)\n",
    "        result.append(str(value))\n",
    "        assert i == value\n",
    "    print('5 elements: ', ','.join(result))\n",
    "    result.clear()\n",
    "    \n",
    "    # Initialize the same iterator over a dataset with 10 elements.\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "    for i in range(10):\n",
    "        value = sess.run(next_element)\n",
    "        result.append(str(value))\n",
    "        assert i == value\n",
    "    print('10 elements: ', ','.join(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>◎ reinitializable iterator</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>A <font color=green>**reinitializable iterator**</font> can be initialized from multiple different **`Dataset`** objects.</font> For example, you might have a <font color=blue>training input pipeline</font> that uses _random perturbations_ to the input images to improve generalization, and a <font color=blue>validation input pipeline</font> that evaluates predictions on unmodified data. <br/><br/>These pipelines will typically use different **`Dataset`** objects that have the same structure (i.e. the same types and compatible shapes for each component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] train_set: -5,7,-2,5,-3,2,-3,12,7,16,7,5,5,3,20,16,24,21,13,26,16,23,29,28,25,34,21,19,36,35\n",
      "[0] val_set: 0,1,2,3,4,5,6,7,8,9\n",
      "\n",
      "[1] train_set: 9,-3,-6,4,4,-4,8,-3,-1,16,8,11,5,19,4,6,11,15,12,15,27,26,20,26,30,34,18,33,31,35\n",
      "[1] val_set: 0,1,2,3,4,5,6,7,8,9\n",
      "\n",
      "[2] train_set: -10,10,6,-1,4,5,14,7,0,9,15,1,13,22,11,10,13,18,9,12,15,24,29,22,18,27,31,26,23,23\n",
      "[2] val_set: 0,1,2,3,4,5,6,7,8,9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(30).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# A reinitializable iterator is defined by its structure. We could use the\n",
    "# `output_types` and `output_shapes` properties of either `training_dataset`\n",
    "# or `validation_dataset` here, because they are compatible.\n",
    "iterator = tf.data.Iterator.from_structure(\n",
    "    training_dataset.output_types, \n",
    "    training_dataset.output_shapes\n",
    ")\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run 20 epochs in which the training dataset is traversed, followed by the\n",
    "    # validation dataset.\n",
    "    for i in range(3):\n",
    "        result = []\n",
    "        # Initialize an iterator over the training dataset.\n",
    "        sess.run(training_init_op)\n",
    "        for _ in range(30):\n",
    "            value = sess.run(next_element)\n",
    "            result.append(str(value))\n",
    "        print('[{}] train_set: {}'.format(i, ','.join(result)))\n",
    "        result.clear()\n",
    "\n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(validation_init_op)\n",
    "        for _ in range(10):\n",
    "            value = sess.run(next_element)\n",
    "            result.append(str(value))\n",
    "        print('[{}] val_set: {}'.format(i, ','.join(result)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>◎ feedable iterator</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>A <font color=green>**feedable iterator**</font> can be used together with **`tf.placeholder`** to select what **`Iterator`** to use in each call to **`tf.Session.run`**, via the familiar **`feed_dict`** mechanism.</font> <br/><br/>It offers the same functionality as a <font color=green>**reinitializable iterator**</font>, <font color=red>but it does not require you to initialize the iterator from the start of a dataset when you switch between iterators.</font> For example, using the same training and validation example from above, you can use [**`tf.data.Iterator.from_string_handle`**](https://www.tensorflow.org/api_docs/python/tf/data/Iterator#from_string_handle) to <font color=red>define a feedable iterator that allows you to switch between the two datasets</font>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] train_set: -7,4,-7,7,10,-4,2,8,13,2,1,5,-3,-5,9,14,7,-3,12,5\n",
      "[0] val_set: 0,1,2,3,4\n",
      "\n",
      "[1] train_set: 4,8,9,7,9,9,13,16,3,3,5,9,1,-4,-6,5,7,3,-2,13\n",
      "[1] val_set: 0,1,2,3,4\n",
      "\n",
      "[2] train_set: 8,0,-5,1,4,4,7,2,12,10,9,7,-7,-1,12,-1,14,14,5,-1\n",
      "[2] val_set: 0,1,2,3,4\n",
      "\n",
      "[3] train_set: 5,2,3,8,11,4,5,5,10,12,0,-1,9,3,6,-3,13,-2,0,10\n",
      "[3] val_set: 0,1,2,3,4\n",
      "\n",
      "[4] train_set: 7,-6,2,-6,2,-4,14,5,14,9,9,-2,1,-2,12,0,4,-1,6,15\n",
      "[4] val_set: 0,1,2,3,4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(10).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64)\n",
    ").repeat()\n",
    "validation_dataset = tf.data.Dataset.range(5)\n",
    "\n",
    "# A feedable iterator is defined by a handle placeholder and its structure. We\n",
    "# could use the `output_types` and `output_shapes` properties of either\n",
    "# `training_dataset` or `validation_dataset` here, because they have\n",
    "# identical structure.\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    string_handle=handle, \n",
    "    output_types=training_dataset.output_types, \n",
    "    output_classes=training_dataset.output_shapes\n",
    ")\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# You can use feedable iterators with a variety of different kinds of iterator\n",
    "# (such as one-shot and initializable iterators).\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # The `Iterator.string_handle()` method returns a tensor that\n",
    "    # can be evaluated and used to feed the `handle` placeholder.\n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(validation_iterator.string_handle())\n",
    "\n",
    "    # Loop 5 times, alternating between training and validation.\n",
    "    for i in range(5):\n",
    "        result = []\n",
    "        # Run 200 steps using the training dataset. Note that the training dataset is\n",
    "        # infinite, and we resume from where we left off in the previous `while` loop\n",
    "        # iteration.\n",
    "        for _ in range(20):\n",
    "            value = sess.run(next_element, feed_dict={handle: training_handle})\n",
    "            result.append(str(value))\n",
    "        print('[{}] train_set: {}'.format(i, ','.join(result)))\n",
    "        result.clear()\n",
    "        \n",
    "        # Run one pass over the validation dataset.\n",
    "        sess.run(validation_iterator.initializer)\n",
    "        for _ in range(5):\n",
    "            value = sess.run(next_element, feed_dict={handle: validation_handle})\n",
    "            result.append(str(value))\n",
    "        print('[{}] val_set: {}'.format(i, ','.join(result)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Consuming values from an iterator</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Saving iterator state</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Consuming NumPy arrays</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Consuming TFRecord data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Consuming text data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Consuming CSV data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dta with Dataset.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Parsing tf.Example protocol buffer messages</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Decoding image data and resizing it</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Applying arbitrary Python logic with tf.py_func()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching dataset elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Simple batching</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Batching tensors with padding</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Processing multiple epochs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Randomly shuffling input data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=gray>Using high-level APIs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
